{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hearing-filter",
   "metadata": {},
   "source": [
    "# Base Code\n",
    "\n",
    "Reference\n",
    "\n",
    "- https://github.com/argusswift/YOLOv4-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-controversy",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seeing-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noticed-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.build_model import Build_Model\n",
    "from model.loss.yolo_loss import YoloV4Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disturbed-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "particular-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.datasets as data\n",
    "from utils import cosine_lr_scheduler\n",
    "import utils.data_augment as dataAug\n",
    "import utils.tools as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wired-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.yolov4_config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accredited-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_collate(batch):\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        targets.append(sample[1])\n",
    "    return torch.stack(imgs, 0), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chronic-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# GPU device\n",
    "# Check GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liberal-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "best_mAP = 0.0\n",
    "\n",
    "epochs = 1\n",
    "eval_epoch = 2\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "simplified-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = 'weight/mobilenetv2.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-compensation",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hourly-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tamil-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding box 좌표 변경하기\n",
    "def cvt_coordinate(center_x, center_y, w, h):\n",
    "    '''\n",
    "    start_x, start_y: 시작 꼭짓점 좌표 (x, y)\n",
    "    end_x, end_y: 종료 꼭짓점 좌표 (x, y)\n",
    "    '''\n",
    "    start_x, end_x = center_x - (w//2), center_x + (w//2)\n",
    "    start_y, end_y = center_y - (h//2), center_y + (h//2)\n",
    "\n",
    "    return [start_x, start_y, end_x, end_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-boston",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "analyzed-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildDataset(Dataset):\n",
    "    def __init__(self, path, img_size=416):\n",
    "        self.img_size = img_size\n",
    "        self.classes = [1, 2, 3, 4, 5, 6]\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.class_to_id = dict(zip(self.classes, range(self.num_classes)))\n",
    "        anno_path = path\n",
    "        with open(anno_path, \"r\") as f:\n",
    "            annotations = list(filter(lambda x: len(x) > 0, f.readlines()))\n",
    "        \n",
    "        self.__annotations = annotations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__annotations)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        img_org, bboxes_org = self.__parse_annotation(self.__annotations[item])\n",
    "        img_org = img_org.transpose(2, 0, 1)  # HWC->CHW\n",
    "        \n",
    "        item_mix = random.randint(0, len(self.__annotations) - 1)\n",
    "        img_mix, bboxes_mix = self.__parse_annotation(\n",
    "            self.__annotations[item_mix]\n",
    "        )\n",
    "        img_mix = img_mix.transpose(2, 0, 1)\n",
    "\n",
    "        img, bboxes = dataAug.Mixup()(img_org, bboxes_org, img_mix, bboxes_mix)\n",
    "        del img_org, bboxes_org, img_mix, bboxes_mix\n",
    "\n",
    "        (\n",
    "            label_sbbox,\n",
    "            label_mbbox,\n",
    "            label_lbbox,\n",
    "            sbboxes,\n",
    "            mbboxes,\n",
    "            lbboxes,\n",
    "        ) = self.__creat_label(bboxes)\n",
    "\n",
    "        img = torch.from_numpy(img).float()\n",
    "        label_sbbox = torch.from_numpy(label_sbbox).float()\n",
    "        label_mbbox = torch.from_numpy(label_mbbox).float()\n",
    "        label_lbbox = torch.from_numpy(label_lbbox).float()\n",
    "        sbboxes = torch.from_numpy(sbboxes).float()\n",
    "        mbboxes = torch.from_numpy(mbboxes).float()\n",
    "        lbboxes = torch.from_numpy(lbboxes).float()\n",
    "\n",
    "        return (\n",
    "            img,\n",
    "            label_sbbox,\n",
    "            label_mbbox,\n",
    "            label_lbbox,\n",
    "            sbboxes,\n",
    "            mbboxes,\n",
    "            lbboxes,\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def __parse_annotation(self, annotation):\n",
    "        \"\"\"\n",
    "        Data augument.\n",
    "        :param annotation: Image' path and bboxes' coordinates, categories.\n",
    "        ex. [image_path xmin,ymin,xmax,ymax,class_ind xmin,ymin,xmax,ymax,class_ind ...]\n",
    "        :return: Return the enhanced image and bboxes. bbox'shape is [xmin, ymin, xmax, ymax, class_ind]\n",
    "        \"\"\"\n",
    "        anno = annotation.strip().split(\" \")\n",
    "\n",
    "        img_path = anno[0]\n",
    "        img = cv2.imread(img_path)  # H*W*C and C=BGR\n",
    "        assert img is not None, \"File Not Found \" + img_path\n",
    "        bboxes = np.array(\n",
    "            [list(map(float, box.split(\",\"))) for box in anno[1:]]\n",
    "        )\n",
    "\n",
    "        img, bboxes = dataAug.RandomHorizontalFilp()(\n",
    "            np.copy(img), np.copy(bboxes), img_path\n",
    "        )\n",
    "        img, bboxes = dataAug.RandomCrop()(np.copy(img), np.copy(bboxes))\n",
    "        img, bboxes = dataAug.RandomAffine()(np.copy(img), np.copy(bboxes))\n",
    "        img, bboxes = dataAug.Resize((self.img_size, self.img_size), True)(\n",
    "            np.copy(img), np.copy(bboxes)\n",
    "        )\n",
    "\n",
    "        return img, bboxes\n",
    "    \n",
    "    def __creat_label(self, bboxes):\n",
    "        \"\"\"\n",
    "        Label assignment. For a single picture all GT box bboxes are assigned anchor.\n",
    "        1、Select a bbox in order, convert its coordinates(\"xyxy\") to \"xywh\"; and scale bbox'\n",
    "           xywh by the strides.\n",
    "        2、Calculate the iou between the each detection layer'anchors and the bbox in turn, and select the largest\n",
    "            anchor to predict the bbox.If the ious of all detection layers are smaller than 0.3, select the largest\n",
    "            of all detection layers' anchors to predict the bbox.\n",
    "\n",
    "        Note :\n",
    "        1、The same GT may be assigned to multiple anchors. And the anchors may be on the same or different layer.\n",
    "        2、The total number of bboxes may be more than it is, because the same GT may be assigned to multiple layers\n",
    "        of detection.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        anchors = np.array(cfg.MODEL[\"ANCHORS\"])\n",
    "        strides = np.array(cfg.MODEL[\"STRIDES\"])\n",
    "        train_output_size = self.img_size / strides\n",
    "        anchors_per_scale = cfg.MODEL[\"ANCHORS_PER_SCLAE\"]\n",
    "\n",
    "        label = [\n",
    "            np.zeros(\n",
    "                (\n",
    "                    int(train_output_size[i]),\n",
    "                    int(train_output_size[i]),\n",
    "                    anchors_per_scale,\n",
    "                    6 + self.num_classes,\n",
    "                )\n",
    "            )\n",
    "            for i in range(3)\n",
    "        ]\n",
    "        for i in range(3):\n",
    "            label[i][..., 5] = 1.0\n",
    "\n",
    "        bboxes_xywh = [\n",
    "            np.zeros((150, 4)) for _ in range(3)\n",
    "        ]  # Darknet the max_num is 30\n",
    "        bbox_count = np.zeros((3,))\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            bbox_coor = bbox[:4]\n",
    "            bbox_class_ind = int(bbox[4])\n",
    "            bbox_mix = bbox[5]\n",
    "\n",
    "            # onehot\n",
    "            one_hot = np.zeros(self.num_classes, dtype=np.float32)\n",
    "            one_hot[bbox_class_ind] = 1.0\n",
    "            one_hot_smooth = dataAug.LabelSmooth()(one_hot, self.num_classes)\n",
    "\n",
    "            # convert \"xyxy\" to \"xywh\"\n",
    "            bbox_xywh = np.concatenate(\n",
    "                [\n",
    "                    (bbox_coor[2:] + bbox_coor[:2]) * 0.5,\n",
    "                    bbox_coor[2:] - bbox_coor[:2],\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "            # print(\"bbox_xywh: \", bbox_xywh)\n",
    "\n",
    "            bbox_xywh_scaled = (\n",
    "                1.0 * bbox_xywh[np.newaxis, :] / strides[:, np.newaxis]\n",
    "            )\n",
    "\n",
    "            iou = []\n",
    "            exist_positive = False\n",
    "            for i in range(3):\n",
    "                anchors_xywh = np.zeros((anchors_per_scale, 4))\n",
    "                anchors_xywh[:, 0:2] = (\n",
    "                    np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n",
    "                )  # 0.5 for compensation\n",
    "                anchors_xywh[:, 2:4] = anchors[i]\n",
    "\n",
    "                iou_scale = tools.iou_xywh_numpy(\n",
    "                    bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh\n",
    "                )\n",
    "                iou.append(iou_scale)\n",
    "                iou_mask = iou_scale > 0.3\n",
    "\n",
    "                if np.any(iou_mask):\n",
    "                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(\n",
    "                        np.int32\n",
    "                    )\n",
    "\n",
    "                    # Bug : 当多个bbox对应同一个anchor时，默认将该anchor分配给最后一个bbox\n",
    "                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n",
    "                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n",
    "                    label[i][yind, xind, iou_mask, 5:6] = bbox_mix\n",
    "                    label[i][yind, xind, iou_mask, 6:] = one_hot_smooth\n",
    "\n",
    "                    bbox_ind = int(bbox_count[i] % 150)  # BUG : 150为一个先验值,内存消耗大\n",
    "                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n",
    "                    bbox_count[i] += 1\n",
    "\n",
    "                    exist_positive = True\n",
    "\n",
    "            if not exist_positive:\n",
    "                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
    "                best_detect = int(best_anchor_ind / anchors_per_scale)\n",
    "                best_anchor = int(best_anchor_ind % anchors_per_scale)\n",
    "\n",
    "                xind, yind = np.floor(\n",
    "                    bbox_xywh_scaled[best_detect, 0:2]\n",
    "                ).astype(np.int32)\n",
    "\n",
    "                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n",
    "                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n",
    "                label[best_detect][yind, xind, best_anchor, 5:6] = bbox_mix\n",
    "                label[best_detect][yind, xind, best_anchor, 6:] = one_hot_smooth\n",
    "\n",
    "                bbox_ind = int(bbox_count[best_detect] % 150)\n",
    "                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n",
    "                bbox_count[best_detect] += 1\n",
    "\n",
    "        label_sbbox, label_mbbox, label_lbbox = label\n",
    "        sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
    "\n",
    "        return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "blocked-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_anno_path = './data/train_annotation.txt'\n",
    "train_anno_path = './data/resized_train_annotation.txt'\n",
    "\n",
    "train_dataset = BuildDataset(train_anno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "expired-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "blocked-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Initing MobilenetV2 weights ********************\n",
      "initing Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "initing BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "initing BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "initing BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "initing BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "initing BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "initing BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "**************************************** \n",
      "Loading weight of MobilenetV2 : weight/mobilenetv2.pth\n",
      "Loaded weight of MobilenetV2 : weight/mobilenetv2.pth\n",
      "******************** Initing head_conv weights ********************\n",
      "initing Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "******************** Initing PANet weights ********************\n",
      "initing Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(640, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(16, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(48, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "initing BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "******************** Initing PredictNet weights ********************\n",
      "initing Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(32, 33, kernel_size=(1, 1), stride=(1, 1))\n",
      "initing Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(96, 33, kernel_size=(1, 1), stride=(1, 1))\n",
      "initing Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "initing BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "initing Conv2d(1280, 33, kernel_size=(1, 1), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "yolov4 = Build_Model(weight_path=weight_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "monetary-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(\n",
    "            yolov4.parameters(),\n",
    "            lr=cfg.TRAIN[\"LR_INIT\"],\n",
    "            momentum=cfg.TRAIN[\"MOMENTUM\"],\n",
    "            weight_decay=cfg.TRAIN[\"WEIGHT_DECAY\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "incomplete-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = YoloV4Loss(\n",
    "            anchors=cfg.MODEL[\"ANCHORS\"],\n",
    "            strides=cfg.MODEL[\"STRIDES\"],\n",
    "            iou_threshold_loss=cfg.TRAIN[\"IOU_THRESHOLD_LOSS\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "successful-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = cosine_lr_scheduler.CosineDecayLR(\n",
    "            optimizer,\n",
    "            T_max=epochs * len(train_dataloader),\n",
    "            lr_init=cfg.TRAIN[\"LR_INIT\"],\n",
    "            lr_min=cfg.TRAIN[\"LR_END\"],\n",
    "            warmup=cfg.TRAIN[\"WARMUP_EPOCHS\"] * len(train_dataloader),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-basement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1, step: [0/2321], mloss: tensor([ 768.4109, 2037.0198, 3041.0601, 5846.4902])\n",
      "Epoch: 0/1, step: [10/2321], mloss: tensor([1003.2217, 2070.9287, 4055.7776, 7129.9292])\n",
      "Epoch: 0/1, step: [20/2321], mloss: tensor([1010.4043, 2061.7473, 4084.5840, 7156.7358])\n",
      "Epoch: 0/1, step: [30/2321], mloss: tensor([ 999.3593, 2047.1071, 4092.5608, 7139.0269])\n",
      "Epoch: 0/1, step: [40/2321], mloss: tensor([ 970.4580, 2025.2151, 4049.9153, 7045.5879])\n",
      "Epoch: 0/1, step: [50/2321], mloss: tensor([ 929.6024, 1999.0936, 3908.5303, 6837.2261])\n",
      "Epoch: 0/1, step: [60/2321], mloss: tensor([ 912.2638, 1972.5922, 3703.3408, 6588.1973])\n",
      "Epoch: 0/1, step: [70/2321], mloss: tensor([ 918.7790, 1947.8528, 3582.4097, 6449.0425])\n",
      "Epoch: 0/1, step: [80/2321], mloss: tensor([ 922.9568, 1921.2440, 3485.8484, 6330.0493])\n",
      "Epoch: 0/1, step: [90/2321], mloss: tensor([ 888.6729, 1879.3197, 3238.8723, 6006.8652])\n",
      "Epoch: 0/1, step: [100/2321], mloss: tensor([ 875.5297, 1841.3514, 3036.3794, 5753.2598])\n",
      "Epoch: 0/1, step: [110/2321], mloss: tensor([ 849.3825, 1798.2427, 2833.4602, 5481.0850])\n",
      "Epoch: 0/1, step: [120/2321], mloss: tensor([ 835.5433, 1757.1453, 2655.1528, 5247.8398])\n",
      "Epoch: 0/1, step: [130/2321], mloss: tensor([ 824.2058, 1715.2816, 2516.8025, 5056.2876])\n",
      "Epoch: 0/1, step: [140/2321], mloss: tensor([ 823.2840, 1674.6327, 2406.0127, 4903.9272])\n",
      "Epoch: 0/1, step: [150/2321], mloss: tensor([ 815.1997, 1630.1243, 2298.4946, 4743.8164])\n",
      "Epoch: 0/1, step: [160/2321], mloss: tensor([ 807.3543, 1584.8820, 2191.7939, 4584.0273])\n",
      "Epoch: 0/1, step: [170/2321], mloss: tensor([ 798.0141, 1542.0354, 2091.6072, 4431.6533])\n",
      "Epoch: 0/1, step: [180/2321], mloss: tensor([ 799.8929, 1502.1909, 2002.1534, 4304.2354])\n",
      "Epoch: 0/1, step: [190/2321], mloss: tensor([ 790.2399, 1461.8463, 1914.7869, 4166.8711])\n",
      "Epoch: 0/1, step: [200/2321], mloss: tensor([ 790.0131, 1427.0948, 1840.1921, 4057.2986])\n",
      "Epoch: 0/1, step: [210/2321], mloss: tensor([ 787.9562, 1390.2953, 1770.8254, 3949.0750])\n",
      "Epoch: 0/1, step: [220/2321], mloss: tensor([ 785.9995, 1354.3964, 1702.5109, 3842.9048])\n",
      "Epoch: 0/1, step: [230/2321], mloss: tensor([ 788.9297, 1320.7054, 1643.2080, 3752.8406])\n",
      "Epoch: 0/1, step: [240/2321], mloss: tensor([ 780.3406, 1288.1292, 1591.6770, 3660.1445])\n",
      "Epoch: 0/1, step: [250/2321], mloss: tensor([ 777.0856, 1258.1984, 1545.5913, 3580.8735])\n",
      "Epoch: 0/1, step: [260/2321], mloss: tensor([ 782.7078, 1230.0424, 1499.1635, 3511.9121])\n",
      "Epoch: 0/1, step: [270/2321], mloss: tensor([ 777.2396, 1202.7534, 1463.3060, 3443.2974])\n",
      "Epoch: 0/1, step: [280/2321], mloss: tensor([ 773.9108, 1176.7059, 1425.1808, 3375.7957])\n",
      "Epoch: 0/1, step: [290/2321], mloss: tensor([ 773.5255, 1151.1567, 1384.4614, 3309.1406])\n",
      "Epoch: 0/1, step: [300/2321], mloss: tensor([ 772.7450, 1125.4865, 1343.5811, 3241.8098])\n",
      "Epoch: 0/1, step: [310/2321], mloss: tensor([ 767.0566, 1100.3479, 1305.0992, 3172.5007])\n",
      "Epoch: 0/1, step: [320/2321], mloss: tensor([ 767.6265, 1077.4464, 1272.3088, 3117.3789])\n",
      "Epoch: 0/1, step: [330/2321], mloss: tensor([ 762.9187, 1054.1802, 1237.6625, 3054.7583])\n",
      "Epoch: 0/1, step: [340/2321], mloss: tensor([ 764.1345, 1032.4855, 1206.7629, 3003.3799])\n",
      "Epoch: 0/1, step: [350/2321], mloss: tensor([ 764.9922, 1012.7792, 1179.1482, 2956.9170])\n",
      "Epoch: 0/1, step: [360/2321], mloss: tensor([ 762.4841,  992.2113, 1152.0372, 2906.7307])\n",
      "Epoch: 0/1, step: [370/2321], mloss: tensor([ 765.5844,  973.6492, 1127.3079, 2866.5398])\n",
      "Epoch: 0/1, step: [380/2321], mloss: tensor([ 763.4262,  955.9426, 1102.3314, 2821.6992])\n",
      "Epoch: 0/1, step: [390/2321], mloss: tensor([ 761.9891,  939.7875, 1080.5337, 2782.3091])\n",
      "Epoch: 0/1, step: [400/2321], mloss: tensor([ 761.1904,  923.0994, 1057.4268, 2741.7153])\n",
      "Epoch: 0/1, step: [410/2321], mloss: tensor([ 760.5861,  907.2673, 1035.5081, 2703.3611])\n",
      "Epoch: 0/1, step: [420/2321], mloss: tensor([ 758.9570,  891.0331, 1014.2462, 2664.2354])\n",
      "Epoch: 0/1, step: [430/2321], mloss: tensor([ 756.9273,  877.6693,  996.5464, 2631.1418])\n",
      "Epoch: 0/1, step: [440/2321], mloss: tensor([ 760.4999,  863.4370,  978.2065, 2602.1421])\n",
      "Epoch: 0/1, step: [450/2321], mloss: tensor([ 761.0382,  850.7552,  961.6829, 2573.4749])\n",
      "Epoch: 0/1, step: [460/2321], mloss: tensor([ 761.2976,  837.7603,  944.5132, 2543.5696])\n",
      "Epoch: 0/1, step: [470/2321], mloss: tensor([ 762.9830,  826.1671,  929.0283, 2518.1768])\n",
      "Epoch: 0/1, step: [480/2321], mloss: tensor([ 765.6442,  814.2542,  913.3323, 2493.2290])\n",
      "Epoch: 0/1, step: [490/2321], mloss: tensor([ 766.0891,  802.6036,  898.0073, 2466.6980])\n",
      "Epoch: 0/1, step: [500/2321], mloss: tensor([ 764.3640,  791.3647,  882.7382, 2438.4651])\n",
      "Epoch: 0/1, step: [510/2321], mloss: tensor([ 765.7762,  781.4304,  869.2933, 2416.4983])\n",
      "Epoch: 0/1, step: [520/2321], mloss: tensor([ 767.8345,  772.8827,  858.2576, 2398.9734])\n",
      "Epoch: 0/1, step: [530/2321], mloss: tensor([ 767.3788,  762.3671,  844.8331, 2374.5771])\n",
      "Epoch: 0/1, step: [540/2321], mloss: tensor([ 764.0948,  751.4263,  831.1195, 2346.6387])\n",
      "Epoch: 0/1, step: [550/2321], mloss: tensor([ 765.0461,  742.7622,  819.7278, 2327.5344])\n",
      "Epoch: 0/1, step: [560/2321], mloss: tensor([ 765.3840,  733.5343,  807.8887, 2306.8057])\n",
      "Epoch: 0/1, step: [570/2321], mloss: tensor([ 763.4259,  724.5674,  796.0505, 2284.0425])\n",
      "Epoch: 0/1, step: [580/2321], mloss: tensor([ 764.5222,  717.1262,  785.9161, 2267.5632])\n",
      "Epoch: 0/1, step: [590/2321], mloss: tensor([ 766.8835,  709.9700,  775.7051, 2252.5569])\n",
      "Epoch: 0/1, step: [600/2321], mloss: tensor([ 766.7548,  703.1640,  766.3480, 2236.2654])\n",
      "Epoch: 0/1, step: [610/2321], mloss: tensor([ 763.5816,  694.8076,  755.9542, 2214.3416])\n",
      "Epoch: 0/1, step: [620/2321], mloss: tensor([ 759.9726,  686.3675,  745.7400, 2192.0786])\n",
      "Epoch: 0/1, step: [630/2321], mloss: tensor([ 757.9835,  678.6263,  735.7483, 2172.3569])\n",
      "Epoch: 0/1, step: [640/2321], mloss: tensor([ 760.1546,  672.0366,  727.1526, 2159.3430])\n",
      "Epoch: 0/1, step: [650/2321], mloss: tensor([ 760.7966,  665.5643,  718.8526, 2145.2129])\n",
      "Epoch: 0/1, step: [660/2321], mloss: tensor([ 759.4513,  658.7617,  710.3607, 2128.5732])\n",
      "Epoch: 0/1, step: [670/2321], mloss: tensor([ 755.9396,  652.1769,  701.5461, 2109.6624])\n",
      "Epoch: 0/1, step: [680/2321], mloss: tensor([ 754.4633,  644.7584,  692.9101, 2092.1321])\n",
      "Epoch: 0/1, step: [690/2321], mloss: tensor([ 754.1014,  638.9953,  685.1621, 2078.2593])\n",
      "Epoch: 0/1, step: [700/2321], mloss: tensor([ 754.3278,  633.0038,  677.7597, 2065.0916])\n",
      "Epoch: 0/1, step: [710/2321], mloss: tensor([ 754.9605,  629.1038,  671.4744, 2055.5386])\n",
      "Epoch: 0/1, step: [720/2321], mloss: tensor([ 753.2810,  623.7378,  664.4328, 2041.4517])\n",
      "Epoch: 0/1, step: [730/2321], mloss: tensor([ 752.6252,  618.5676,  657.3953, 2028.5883])\n",
      "Epoch: 0/1, step: [740/2321], mloss: tensor([ 753.1796,  613.3295,  651.0459, 2017.5551])\n",
      "Epoch: 0/1, step: [750/2321], mloss: tensor([ 753.9368,  607.8407,  644.5840, 2006.3616])\n",
      "Epoch: 0/1, step: [760/2321], mloss: tensor([ 752.1425,  602.7410,  637.8911, 1992.7743])\n",
      "Epoch: 0/1, step: [770/2321], mloss: tensor([ 751.9009,  598.1630,  631.7811, 1981.8448])\n",
      "Epoch: 0/1, step: [780/2321], mloss: tensor([ 748.8999,  593.1183,  625.1791, 1967.1970])\n",
      "Epoch: 0/1, step: [790/2321], mloss: tensor([ 749.9996,  589.0608,  619.9558, 1959.0161])\n",
      "Epoch: 0/1, step: [800/2321], mloss: tensor([ 749.9305,  584.5798,  614.0878, 1948.5980])\n",
      "Epoch: 0/1, step: [810/2321], mloss: tensor([ 747.3580,  579.8260,  607.8517, 1935.0358])\n",
      "Epoch: 0/1, step: [820/2321], mloss: tensor([ 746.6425,  575.3455,  602.0532, 1924.0410])\n",
      "Epoch: 0/1, step: [830/2321], mloss: tensor([ 746.6522,  570.4990,  596.3529, 1913.5039])\n",
      "Epoch: 0/1, step: [840/2321], mloss: tensor([ 745.6324,  565.9846,  590.9753, 1902.5923])\n",
      "Epoch: 0/1, step: [850/2321], mloss: tensor([ 746.6706,  561.5798,  585.9134, 1894.1639])\n",
      "Epoch: 0/1, step: [860/2321], mloss: tensor([ 748.5643,  557.4920,  580.8171, 1886.8740])\n",
      "Epoch: 0/1, step: [870/2321], mloss: tensor([ 750.9632,  553.5916,  576.3555, 1880.9108])\n",
      "Epoch: 0/1, step: [880/2321], mloss: tensor([ 751.1946,  549.8857,  571.6119, 1872.6925])\n",
      "Epoch: 0/1, step: [890/2321], mloss: tensor([ 750.6339,  545.7526,  566.4186, 1862.8055])\n",
      "Epoch: 0/1, step: [900/2321], mloss: tensor([ 750.1320,  542.3663,  561.5886, 1854.0874])\n",
      "Epoch: 0/1, step: [910/2321], mloss: tensor([ 750.8451,  539.0795,  557.2579, 1847.1829])\n",
      "Epoch: 0/1, step: [920/2321], mloss: tensor([ 750.7004,  535.7964,  552.9927, 1839.4899])\n",
      "Epoch: 0/1, step: [930/2321], mloss: tensor([ 749.8516,  532.9996,  548.5693, 1831.4207])\n",
      "Epoch: 0/1, step: [940/2321], mloss: tensor([ 749.3891,  530.3337,  544.8706, 1824.5935])\n",
      "Epoch: 0/1, step: [950/2321], mloss: tensor([ 747.0183,  527.3187,  540.5109, 1814.8479])\n",
      "Epoch: 0/1, step: [960/2321], mloss: tensor([ 743.3121,  523.3586,  535.7094, 1802.3799])\n",
      "Epoch: 0/1, step: [970/2321], mloss: tensor([ 743.0026,  519.8977,  531.4724, 1794.3727])\n",
      "Epoch: 0/1, step: [980/2321], mloss: tensor([ 743.5289,  516.6132,  527.9667, 1788.1086])\n",
      "Epoch: 0/1, step: [990/2321], mloss: tensor([ 742.5496,  512.9116,  523.9786, 1779.4399])\n",
      "Epoch: 0/1, step: [1000/2321], mloss: tensor([ 741.1265,  509.1736,  520.0241, 1770.3245])\n",
      "Epoch: 0/1, step: [1010/2321], mloss: tensor([ 740.4439,  507.8503,  516.8566, 1765.1511])\n",
      "Epoch: 0/1, step: [1020/2321], mloss: tensor([ 741.2730,  505.4730,  513.2650, 1760.0114])\n",
      "Epoch: 0/1, step: [1030/2321], mloss: tensor([ 739.6353,  504.4598,  510.8684, 1754.9640])\n",
      "Epoch: 0/1, step: [1040/2321], mloss: tensor([ 739.4382,  502.4514,  508.2065, 1750.0966])\n",
      "Epoch: 0/1, step: [1050/2321], mloss: tensor([ 737.9157,  499.5327,  504.4667, 1741.9155])\n",
      "Epoch: 0/1, step: [1060/2321], mloss: tensor([ 738.3314,  496.6848,  500.9433, 1735.9597])\n",
      "Epoch: 0/1, step: [1070/2321], mloss: tensor([ 739.4348,  494.2686,  497.6910, 1731.3947])\n",
      "Epoch: 0/1, step: [1080/2321], mloss: tensor([ 740.6274,  491.4669,  494.4228, 1726.5176])\n",
      "Epoch: 0/1, step: [1090/2321], mloss: tensor([ 740.7348,  488.9090,  491.3990, 1721.0436])\n",
      "Epoch: 0/1, step: [1100/2321], mloss: tensor([ 740.1041,  485.9385,  487.9654, 1714.0088])\n",
      "Epoch: 0/1, step: [1110/2321], mloss: tensor([ 737.7767,  482.9319,  484.6383, 1705.3478])\n",
      "Epoch: 0/1, step: [1120/2321], mloss: tensor([ 737.6818,  480.3256,  481.4954, 1699.5033])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    yolov4.train()\n",
    "    \n",
    "    mloss = torch.zeros(4)\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        scheduler.step(\n",
    "                    len(train_dataloader)\n",
    "                    / (cfg.TRAIN[\"BATCH_SIZE\"])\n",
    "                    * epoch\n",
    "                    + i\n",
    "                )\n",
    "        \n",
    "        imgs = data[0]\n",
    "        label_sbbox = data[1]\n",
    "        label_mbbox = data[2]\n",
    "        label_lbbox = data[3]\n",
    "        sbboxes = data[4]\n",
    "        mbboxes = data[5]\n",
    "        lbboxes = data[6]\n",
    "        \n",
    "        imgs = imgs.to(device)\n",
    "        label_sbbox = label_sbbox.to(device)\n",
    "        label_mbbox = label_mbbox.to(device)\n",
    "        label_lbbox = label_lbbox.to(device)\n",
    "        sbboxes = sbboxes.to(device)\n",
    "        mbboxes = mbboxes.to(device)\n",
    "        lbboxes = lbboxes.to(device)\n",
    "        \n",
    "#         plt.imshow(np.transpose(imgs.squeeze(), (1, 2, 0)))\n",
    "#         plt.show()\n",
    "        \n",
    "        p, p_d = yolov4(imgs)\n",
    "\n",
    "        loss, loss_ciou, loss_conf, loss_cls = criterion(\n",
    "            p,\n",
    "            p_d,\n",
    "            label_sbbox,\n",
    "            label_mbbox,\n",
    "            label_lbbox,\n",
    "            sbboxes,\n",
    "            mbboxes,\n",
    "            lbboxes,\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        loss_items = torch.tensor([loss_ciou, loss_conf, loss_cls, loss])\n",
    "        mloss = (mloss * i + loss_items) / (i + 1)\n",
    "        \n",
    "#         print(f\"epoch: {epoch}, mloss: {mloss}\")\n",
    "        if i%10 == 0:\n",
    "            print(f\"Epoch: {epoch}/{epochs}, step: [{i}/{len(train_dataloader) - 1}], mloss: {mloss}\")\n",
    "#             print(f\"=== Epoch:[{epoch}/{epochs}], step:[{i}/{len(self.train_dataloader) - 1}], img_size:[{train_dataset.img_size:3}], total_loss:{mloss[3]:.4f}|loss_ciou:{mloss[0]:.4f}|loss_conf:{mloss[1]:.4f}|loss_cls:{mloss[2]:.4f}|lr:{optimizer.param_groups[0][\"lr\"]:.6f}\")\n",
    "        \n",
    "        chkpt = {\n",
    "            \"epoch\": epoch,\n",
    "            # \"best_mAP\": self.best_mAP,\n",
    "            \"model\": yolov4.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(chkpt, './weight/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-senate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "intense-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = os.listdir('./data/img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "standard-frost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'58866d7ee4b08153af63ac53.jpg' in lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-contributor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

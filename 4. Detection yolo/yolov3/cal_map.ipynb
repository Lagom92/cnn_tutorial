{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mAP 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Fast/er R-CNN\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Bharath Hariharan\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# def parse_rec(filename):\n",
    "#     \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
    "#     tree = ET.parse(filename)\n",
    "#     objects = []\n",
    "#     for obj in tree.findall('object'):\n",
    "#         obj_struct = {}\n",
    "#         obj_struct['name'] = obj.find('name').text\n",
    "#         obj_struct['pose'] = obj.find('pose').text\n",
    "#         obj_struct['truncated'] = int(obj.find('truncated').text)\n",
    "#         obj_struct['difficult'] = int(obj.find('difficult').text)\n",
    "#         bbox = obj.find('bndbox')\n",
    "#         obj_struct['bbox'] = [int(bbox.find('xmin').text),\n",
    "#                               int(bbox.find('ymin').text),\n",
    "#                               int(bbox.find('xmax').text),\n",
    "#                               int(bbox.find('ymax').text)]\n",
    "#         objects.append(obj_struct)\n",
    "\n",
    "#     return objects\n",
    "\n",
    "def voc_ap(rec, prec, use_07_metric=False):\n",
    "    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n",
    "    Compute VOC AP given precision and recall.\n",
    "    If use_07_metric is true, uses the\n",
    "    VOC 07 11 point method (default:False).\n",
    "    \"\"\"\n",
    "    if use_07_metric:\n",
    "        # 11 point metric\n",
    "        ap = 0.\n",
    "        for t in np.arange(0., 1.1, 0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(prec[rec >= t])\n",
    "            ap = ap + p / 11.\n",
    "    else:\n",
    "        # correct AP calculation\n",
    "        # first append sentinel values at the end\n",
    "        mrec = np.concatenate(([0.], rec, [1.]))\n",
    "        mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "        # compute the precision envelope\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "        # to calculate area under PR curve, look for points\n",
    "        # where X axis (recall) changes value\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "        # and sum (\\Delta recall) * prec\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "def voc_eval(detpath,\n",
    "             annopath,\n",
    "             imagesetfile,\n",
    "             classname,\n",
    "             cachedir,\n",
    "             ovthresh=0.5,\n",
    "             use_07_metric=False):\n",
    "    \"\"\"rec, prec, ap = voc_eval(detpath,\n",
    "                                annopath,\n",
    "                                imagesetfile,\n",
    "                                classname,\n",
    "                                [ovthresh],\n",
    "                                [use_07_metric])\n",
    "\n",
    "    Top level function that does the PASCAL VOC evaluation.\n",
    "\n",
    "    detpath: Path to detections\n",
    "        detpath.format(classname) should produce the detection results file.\n",
    "    annopath: Path to annotations\n",
    "        annopath.format(imagename) should be the xml annotations file.\n",
    "    imagesetfile: Text file containing the list of images, one image per line.\n",
    "    classname: Category name (duh)\n",
    "    cachedir: Directory for caching the annotations\n",
    "    [ovthresh]: Overlap threshold (default = 0.5)\n",
    "    [use_07_metric]: Whether to use VOC07's 11 point AP computation\n",
    "        (default False)\n",
    "    \"\"\"\n",
    "    # assumes detections are in detpath.format(classname)\n",
    "    # assumes annotations are in annopath.format(imagename)\n",
    "    # assumes imagesetfile is a text file with each line an image name\n",
    "    # cachedir caches the annotations in a pickle file\n",
    "\n",
    "    # first load gt\n",
    "    if not os.path.isdir(cachedir):\n",
    "        os.mkdir(cachedir)\n",
    "    cachefile = os.path.join(cachedir, 'annots.pkl')\n",
    "    # read list of images\n",
    "    with open(imagesetfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    imagenames = [x.strip() for x in lines]\n",
    "\n",
    "    if not os.path.isfile(cachefile):\n",
    "        # load annots\n",
    "        recs = {}\n",
    "        for i, imagename in enumerate(imagenames):\n",
    "            recs[imagename] = parse_rec(annopath.format(imagename))\n",
    "            if i % 100 == 0:\n",
    "                print ('Reading annotation for {:d}/{:d}'.format(\n",
    "                    i + 1, len(imagenames)))\n",
    "        # save\n",
    "        print ('Saving cached annotations to {:s}'.format(cachefile))\n",
    "        with open(cachefile, 'wb') as f:\n",
    "            pickle.dump(recs, f)\n",
    "    else:\n",
    "        # load\n",
    "        with open(cachefile, 'rb') as f:\n",
    "            recs = pickle.load(f)\n",
    "\n",
    "    # extract gt objects for this class\n",
    "    class_recs = {}\n",
    "    npos = 0\n",
    "    for imagename in imagenames:\n",
    "        R = [obj for obj in recs[imagename] if obj['name'] == classname]\n",
    "        bbox = np.array([x['bbox'] for x in R])\n",
    "        difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n",
    "        det = [False] * len(R)\n",
    "        npos = npos + sum(~difficult)\n",
    "        class_recs[imagename] = {'bbox': bbox,\n",
    "                                 'difficult': difficult,\n",
    "                                 'det': det}\n",
    "\n",
    "    # read dets\n",
    "    detfile = detpath.format(classname)\n",
    "    with open(detfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    splitlines = [x.strip().split(' ') for x in lines]\n",
    "    image_ids = [x[0] for x in splitlines]\n",
    "    confidence = np.array([float(x[1]) for x in splitlines])\n",
    "    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n",
    "\n",
    "    # sort by confidence\n",
    "    sorted_ind = np.argsort(-confidence)\n",
    "    sorted_scores = np.sort(-confidence)\n",
    "    BB = BB[sorted_ind, :]\n",
    "    image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "    # go down dets and mark TPs and FPs\n",
    "    nd = len(image_ids)\n",
    "    tp = np.zeros(nd)\n",
    "    fp = np.zeros(nd)\n",
    "    for d in range(nd):\n",
    "        R = class_recs[image_ids[d]]\n",
    "        bb = BB[d, :].astype(float)\n",
    "        ovmax = -np.inf\n",
    "        BBGT = R['bbox'].astype(float)\n",
    "\n",
    "        if BBGT.size > 0:\n",
    "            # compute overlaps\n",
    "            # intersection\n",
    "            ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
    "            iymin = np.maximum(BBGT[:, 1], bb[1])\n",
    "            ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
    "            iymax = np.minimum(BBGT[:, 3], bb[3])\n",
    "            iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "            ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "            inters = iw * ih\n",
    "\n",
    "            # union\n",
    "            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n",
    "                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n",
    "                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n",
    "\n",
    "            overlaps = inters / uni\n",
    "            ovmax = np.max(overlaps)\n",
    "            jmax = np.argmax(overlaps)\n",
    "\n",
    "        if ovmax > ovthresh:\n",
    "            if not R['difficult'][jmax]:\n",
    "                if not R['det'][jmax]:\n",
    "                    tp[d] = 1.\n",
    "                    R['det'][jmax] = 1\n",
    "                else:\n",
    "                    fp[d] = 1.\n",
    "        else:\n",
    "            fp[d] = 1.\n",
    "\n",
    "    # compute precision recall\n",
    "    fp = np.cumsum(fp)\n",
    "    tp = np.cumsum(tp)\n",
    "    rec = tp / float(npos)\n",
    "    # avoid divide by zero in case the first detection matches a difficult\n",
    "    # ground truth\n",
    "    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "    ap = voc_ap(rec, prec, use_07_metric)\n",
    "\n",
    "    return rec, prec, ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.yolov3_config_voc as cfg\n",
    "import os\n",
    "import shutil\n",
    "# from eval import voc_eval\n",
    "from utils.datasets import *\n",
    "from utils.gpu import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils.data_augment import *\n",
    "import torch\n",
    "from utils.tools import *\n",
    "from tqdm import tqdm\n",
    "from utils.visualize import *\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self, model, visiual=True):\n",
    "        self.classes = cfg.DATA[\"CLASSES\"]\n",
    "        self.pred_result_path = os.path.join(cfg.PROJECT_PATH, 'data', 'results')\n",
    "#         self.val_data_path = os.path.join(cfg.DATA_PATH, 'VOCtest-2007', 'VOCdevkit', 'VOC2007')\n",
    "        self.conf_thresh = cfg.TEST[\"CONF_THRESH\"]\n",
    "        self.nms_thresh = cfg.TEST[\"NMS_THRESH\"]\n",
    "        self.val_shape =  cfg.TEST[\"TEST_IMG_SIZE\"]\n",
    "\n",
    "        self.__visiual = visiual\n",
    "        self.__visual_imgs = 0\n",
    "\n",
    "        self.model = model\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    def APs_voc(self):\n",
    "        path = './data/test/'\n",
    "        img_list = glob.glob(path + '*.jpg')\n",
    "\n",
    "        if os.path.exists(self.pred_result_path):\n",
    "            shutil.rmtree(self.pred_result_path)\n",
    "        os.mkdir(self.pred_result_path)\n",
    "\n",
    "        for img_path in tqdm(img_list[:10]):\n",
    "            img_title = img_path.split('/')[-1]\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            bboxes_prd = self.get_bbox(img)\n",
    "  \n",
    "            if bboxes_prd.shape[0]!=0 and self.__visiual and self.__visual_imgs < 100:\n",
    "                boxes = bboxes_prd[..., :4]\n",
    "                class_inds = bboxes_prd[..., 5].astype(np.int32)\n",
    "                scores = bboxes_prd[..., 4]\n",
    "\n",
    "                visualize_boxes(image=img, boxes=boxes, labels=class_inds, probs=scores, class_labels=self.classes)\n",
    "                path = os.path.join(cfg.PROJECT_PATH, \"data/results/{}.jpg\".format(self.__visual_imgs))\n",
    "                cv2.imwrite(path, img)\n",
    "\n",
    "                self.__visual_imgs += 1\n",
    "\n",
    "            for bbox in bboxes_prd:\n",
    "                coor = np.array(bbox[:4], dtype=np.int32)\n",
    "                score = bbox[4]\n",
    "                class_ind = int(bbox[5])\n",
    "\n",
    "                class_name = self.classes[class_ind]\n",
    "                score = '%.4f' % score\n",
    "                xmin, ymin, xmax, ymax = map(str, coor)\n",
    "                s = ' '.join([img_title, score, xmin, ymin, xmax, ymax]) + '\\n'\n",
    "\n",
    "                with open(os.path.join(self.pred_result_path, 'comp4_det_test_' + class_name + '.txt'), 'a') as f:\n",
    "                    f.write(s)\n",
    "\n",
    "        return self.__calc_APs()\n",
    "\n",
    "    def get_bbox(self, img):\n",
    "        \n",
    "        bboxes = self.__predict(img, self.val_shape, (0, np.inf))\n",
    "        bboxes = nms(bboxes, self.conf_thresh, self.nms_thresh)\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    def __predict(self, img, test_shape, valid_scale):\n",
    "        org_img = np.copy(img)\n",
    "        org_h, org_w, _ = org_img.shape\n",
    "\n",
    "        img = self.__get_img_tensor(img, test_shape).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, p_d = self.model(img)\n",
    "        pred_bbox = p_d.squeeze().cpu().numpy()\n",
    "        bboxes = self.__convert_pred(pred_bbox, test_shape, (org_h, org_w), valid_scale)\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    def __get_img_tensor(self, img, test_shape):\n",
    "        img = Resize((test_shape, test_shape), correct_box=False)(img, None).transpose(2, 0, 1)\n",
    "        return torch.from_numpy(img[np.newaxis, ...]).float()\n",
    "\n",
    "\n",
    "    def __convert_pred(self, pred_bbox, test_input_size, org_img_shape, valid_scale):\n",
    "        \"\"\"\n",
    "        预测框进行过滤，去除尺度不合理的框\n",
    "        \"\"\"\n",
    "        pred_coor = xywh2xyxy(pred_bbox[:, :4])\n",
    "        pred_conf = pred_bbox[:, 4]\n",
    "        pred_prob = pred_bbox[:, 5:]\n",
    "\n",
    "        # (1)\n",
    "        # (xmin_org, xmax_org) = ((xmin, xmax) - dw) / resize_ratio\n",
    "        # (ymin_org, ymax_org) = ((ymin, ymax) - dh) / resize_ratio\n",
    "        # 需要注意的是，无论我们在训练的时候使用什么数据增强方式，都不影响此处的转换方式\n",
    "        # 假设我们对输入测试图片使用了转换方式A，那么此处对bbox的转换方式就是方式A的逆向过程\n",
    "        org_h, org_w = org_img_shape\n",
    "        resize_ratio = min(1.0 * test_input_size / org_w, 1.0 * test_input_size / org_h)\n",
    "        dw = (test_input_size - resize_ratio * org_w) / 2\n",
    "        dh = (test_input_size - resize_ratio * org_h) / 2\n",
    "        pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n",
    "        pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n",
    "\n",
    "        # (2)将预测的bbox中超出原图的部分裁掉\n",
    "        pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n",
    "                                    np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n",
    "        # (3)将无效bbox的coor置为0\n",
    "        invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n",
    "        pred_coor[invalid_mask] = 0\n",
    "\n",
    "        # (4)去掉不在有效范围内的bbox\n",
    "        bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n",
    "        scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n",
    "\n",
    "        # (5)将score低于score_threshold的bbox去掉\n",
    "        classes = np.argmax(pred_prob, axis=-1)\n",
    "        scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n",
    "        score_mask = scores > self.conf_thresh\n",
    "\n",
    "        mask = np.logical_and(scale_mask, score_mask)\n",
    "\n",
    "        coors = pred_coor[mask]\n",
    "        scores = scores[mask]\n",
    "        classes = classes[mask]\n",
    "\n",
    "        bboxes = np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "\n",
    "    def __calc_APs(self, iou_thresh=0.5, use_07_metric=False):\n",
    "        \"\"\"\n",
    "        计算每个类别的ap值\n",
    "        :param iou_thresh:\n",
    "        :param use_07_metric:\n",
    "        :return:dict{cls:ap}\n",
    "        \"\"\"\n",
    "        filename = os.path.join(self.pred_result_path, 'comp4_det_test_{:s}.txt')\n",
    "        cachedir = os.path.join(self.pred_result_path, 'cache')\n",
    "#         annopath = os.path.join(self.val_data_path, 'Annotations', '{:s}.xml')\n",
    "        imagesetfile = './data/test/'\n",
    "        APs = {}\n",
    "        for i, cls in enumerate(self.classes):\n",
    "            R, P, AP = voc_eval(filename, annopath, imagesetfile, cls, cachedir, iou_thresh, use_07_metric)\n",
    "#             R, P, AP = voc_eval.voc_eval(filename, annopath, imagesetfile, cls, cachedir, iou_thresh, use_07_metric)\n",
    "            APs[cls] = AP\n",
    "        if os.path.exists(cachedir):\n",
    "            shutil.rmtree(cachedir)\n",
    "\n",
    "        return APs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import utils.gpu as gpu\n",
    "from model.yolov3 import Yolov3\n",
    "from tqdm import tqdm\n",
    "from utils.tools import *\n",
    "# from eval.evaluator import Evaluator\n",
    "import argparse\n",
    "import config.yolov3_config_voc as cfg\n",
    "from utils.visualize import *\n",
    "\n",
    "\n",
    "class Tester(object):\n",
    "    def __init__(self,\n",
    "                 weight_path='./weight/best.pt',\n",
    "                 gpu_id=0,\n",
    "                 img_size=416,\n",
    "                 visiual='./data/test/',\n",
    "                 eval=True\n",
    "                 ):\n",
    "        self.img_size = img_size\n",
    "        self.__num_class = cfg.DATA[\"NUM\"]\n",
    "        self.__conf_threshold = cfg.TEST[\"CONF_THRESH\"]\n",
    "        self.__nms_threshold = cfg.TEST[\"NMS_THRESH\"]\n",
    "        self.__device = gpu.select_device(gpu_id)\n",
    "        self.__multi_scale_test = cfg.TEST[\"MULTI_SCALE_TEST\"]\n",
    "        self.__flip_test = cfg.TEST[\"FLIP_TEST\"]\n",
    "\n",
    "        self.__visiual = visiual\n",
    "        self.__eval = eval\n",
    "        self.__classes = cfg.DATA[\"CLASSES\"]\n",
    "\n",
    "        self.__model = Yolov3().to(self.__device)\n",
    "\n",
    "        # self.__load_model_weights(weight_path)\n",
    "        self.__load_model_weights(weight_path)\n",
    "\n",
    "#         self.__evalter = Evaluator(self.__model, visiual=True)\n",
    "        self.__evalter = Evaluator(self.__model)\n",
    "\n",
    "\n",
    "    def __load_model_weights(self, weight_path):\n",
    "#         print(\"loading weight file from : {}\".format(weight_path))\n",
    "\n",
    "        weight = os.path.join(weight_path)\n",
    "        chkpt = torch.load(weight, map_location=self.__device)\n",
    "        self.__model.load_state_dict(chkpt['model'])\n",
    "#         self.__model.load_state_dict(chkpt)\n",
    "#         print(\"loading weight file is done\")\n",
    "        del chkpt\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "#         if self.__visiual:\n",
    "#             imgs = os.listdir(self.__visiual)\n",
    "#             for v in imgs:\n",
    "#                 path = os.path.join(self.__visiual, v)\n",
    "# #                 print(\"test images : {}\".format(path))\n",
    "#                 img = cv2.imread(path)\n",
    "#                 assert img is not None\n",
    "#                 bboxes_prd = self.__evalter.get_bbox(img)\n",
    "#                 if bboxes_prd.shape[0] != 0:\n",
    "#                     boxes = bboxes_prd[..., :4]\n",
    "#                     class_inds = bboxes_prd[..., 5].astype(np.int32)\n",
    "#                     scores = bboxes_prd[..., 4]\n",
    "\n",
    "#                     visualize_boxes(image=img, boxes=boxes, labels=class_inds, probs=scores, class_labels=self.__classes)\n",
    "#                     path = os.path.join(cfg.PROJECT_PATH, \"data/pred/{}\".format(v))\n",
    "\n",
    "#                     cv2.imwrite(path, img)\n",
    "#                     print(\"saved images : {}\".format(path))\n",
    "\n",
    "        print(\"eval start !\")\n",
    "        if self.__eval:\n",
    "            mAP = 0\n",
    "            print('*' * 20 + \"Validate\" + '*' * 20)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                APs = Evaluator(self.__model).APs_voc()\n",
    "\n",
    "                for i in APs:\n",
    "                    print(\"{} --> mAP : {}\".format(i, APs[i]))\n",
    "                    mAP += APs[i]\n",
    "                mAP = mAP / self.__num_class\n",
    "                print('mAP:%g' % (mAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='TITAN RTX', total_memory=24220MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:00<00:00, 31.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval start !\n",
      "********************Validate********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 31.46it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/VOCtest-2007/VOCdevkit/VOC2007/ImageSets/Main/test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f522931b3ed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-74dcebb8205c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mAPs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPs_voc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAPs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a5523026c20a>\u001b[0m in \u001b[0;36mAPs_voc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calc_APs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a5523026c20a>\u001b[0m in \u001b[0;36m__calc_APs\u001b[0;34m(self, iou_thresh, use_07_metric)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mAPs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannopath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimagesetfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcachedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_07_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;31m#             R, P, AP = voc_eval.voc_eval(filename, annopath, imagesetfile, cls, cachedir, iou_thresh, use_07_metric)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mAPs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-c598a9e57a3f>\u001b[0m in \u001b[0;36mvoc_eval\u001b[0;34m(detpath, annopath, imagesetfile, classname, cachedir, ovthresh, use_07_metric)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mcachefile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcachedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annots.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# read list of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagesetfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mimagenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/VOCtest-2007/VOCdevkit/VOC2007/ImageSets/Main/test.txt'"
     ]
    }
   ],
   "source": [
    "Tester().test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe shape: (228, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>53</td>\n",
       "      <td>188</td>\n",
       "      <td>101</td>\n",
       "      <td>227</td>\n",
       "      <td>56</td>\n",
       "      <td>276</td>\n",
       "      <td>105</td>\n",
       "      <td>156</td>\n",
       "      <td>224</td>\n",
       "      <td>...</td>\n",
       "      <td>273</td>\n",
       "      <td>182</td>\n",
       "      <td>380</td>\n",
       "      <td>211</td>\n",
       "      <td>410</td>\n",
       "      <td>213</td>\n",
       "      <td>380</td>\n",
       "      <td>242</td>\n",
       "      <td>410</td>\n",
       "      <td>./data/test/5d097f4de4b0b6098b782aa0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141</td>\n",
       "      <td>68</td>\n",
       "      <td>189</td>\n",
       "      <td>115</td>\n",
       "      <td>227</td>\n",
       "      <td>68</td>\n",
       "      <td>274</td>\n",
       "      <td>114</td>\n",
       "      <td>167</td>\n",
       "      <td>231</td>\n",
       "      <td>...</td>\n",
       "      <td>273</td>\n",
       "      <td>180</td>\n",
       "      <td>379</td>\n",
       "      <td>209</td>\n",
       "      <td>408</td>\n",
       "      <td>211</td>\n",
       "      <td>379</td>\n",
       "      <td>240</td>\n",
       "      <td>408</td>\n",
       "      <td>./data/test/5d097f71e4b0b6098b782aa5.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>53</td>\n",
       "      <td>187</td>\n",
       "      <td>98</td>\n",
       "      <td>237</td>\n",
       "      <td>52</td>\n",
       "      <td>283</td>\n",
       "      <td>98</td>\n",
       "      <td>141</td>\n",
       "      <td>213</td>\n",
       "      <td>...</td>\n",
       "      <td>259</td>\n",
       "      <td>170</td>\n",
       "      <td>367</td>\n",
       "      <td>202</td>\n",
       "      <td>399</td>\n",
       "      <td>217</td>\n",
       "      <td>367</td>\n",
       "      <td>248</td>\n",
       "      <td>398</td>\n",
       "      <td>./data/test/5d097fa0e4b0b6098b782aa9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>67</td>\n",
       "      <td>186</td>\n",
       "      <td>115</td>\n",
       "      <td>219</td>\n",
       "      <td>67</td>\n",
       "      <td>266</td>\n",
       "      <td>114</td>\n",
       "      <td>150</td>\n",
       "      <td>229</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>173</td>\n",
       "      <td>384</td>\n",
       "      <td>201</td>\n",
       "      <td>412</td>\n",
       "      <td>205</td>\n",
       "      <td>383</td>\n",
       "      <td>232</td>\n",
       "      <td>409</td>\n",
       "      <td>./data/test/5d098290e4b0b6098b782bbe.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "      <td>54</td>\n",
       "      <td>187</td>\n",
       "      <td>102</td>\n",
       "      <td>227</td>\n",
       "      <td>54</td>\n",
       "      <td>275</td>\n",
       "      <td>103</td>\n",
       "      <td>149</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>267</td>\n",
       "      <td>166</td>\n",
       "      <td>384</td>\n",
       "      <td>195</td>\n",
       "      <td>414</td>\n",
       "      <td>218</td>\n",
       "      <td>384</td>\n",
       "      <td>249</td>\n",
       "      <td>415</td>\n",
       "      <td>./data/test/5d099a43e4b05338ae17768d.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1    2    3    4   5    6    7    8    9  ...   15   16   17   18  \\\n",
       "0  140  53  188  101  227  56  276  105  156  224  ...  273  182  380  211   \n",
       "1  141  68  189  115  227  68  274  114  167  231  ...  273  180  379  209   \n",
       "2  141  53  187   98  237  52  283   98  141  213  ...  259  170  367  202   \n",
       "3  137  67  186  115  219  67  266  114  150  229  ...  272  173  384  201   \n",
       "4  139  54  187  102  227  54  275  103  149  220  ...  267  166  384  195   \n",
       "\n",
       "    19   20   21   22   23                                     title  \n",
       "0  410  213  380  242  410  ./data/test/5d097f4de4b0b6098b782aa0.jpg  \n",
       "1  408  211  379  240  408  ./data/test/5d097f71e4b0b6098b782aa5.jpg  \n",
       "2  399  217  367  248  398  ./data/test/5d097fa0e4b0b6098b782aa9.jpg  \n",
       "3  412  205  383  232  409  ./data/test/5d098290e4b0b6098b782bbe.jpg  \n",
       "4  414  218  384  249  415  ./data/test/5d099a43e4b05338ae17768d.jpg  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# annotation\n",
    "df = pd.read_csv('./data/test_annotation.txt', header=None)\n",
    "\n",
    "df['title'] = df[0].str.split(' ').str[0]\n",
    "\n",
    "df[0] = df[0].str.split(' ').str[1]\n",
    "\n",
    "df = df.astype({0:'int64'})\n",
    "\n",
    "print('dataframe shape:', df.shape)\n",
    "\n",
    "# vaalue split and delete class\n",
    "for j in range(df.shape[0]):\n",
    "    for i in range(4, 21, 4):\n",
    "        val = df.iloc[j, i].split(' ')[-1]\n",
    "        df.iloc[j, i] = int(val)\n",
    "        \n",
    "df = df.drop(df.columns[-2], axis=1)       \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "# coordinates\n",
    "org_bboxes = []\n",
    "for i in range(0, 24, 4):\n",
    "    val = df.iloc[idx][i:i+4].tolist()\n",
    "    val = val + [str(i//4 + 1)]\n",
    "    org_bboxes.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[140, 53, 188, 101, '1'],\n",
       " [227, 56, 276, 105, '2'],\n",
       " [156, 224, 202, 270, '3'],\n",
       " [222, 226, 270, 273, '4'],\n",
       " [182, 380, 211, 410, '5'],\n",
       " [213, 380, 242, 410, '6']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:detect]",
   "language": "python",
   "name": "conda-env-detect-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
